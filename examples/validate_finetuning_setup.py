"""
Sistema de ValidaÃ§Ã£o e Teste do Fine-tuning ControlLab
Valida o dataset gerado e prepara ambiente para treinamento
"""

import json
import os
import yaml
from collections import Counter
import matplotlib.pyplot as plt
import pandas as pd
from datetime import datetime

class ControlLabFineTuningValidator:
    def __init__(self):
        self.dataset_path = "C:/Users/marco/Documents/ControlLab/ControlLab-Project/ai_toolkit_dataset/controllab_specialized_dataset.jsonl"
        self.config_path = "C:/Users/marco/Documents/ControlLab/ControlLab-Project/controllab_finetuning_config.yaml"
        self.validation_report_path = "C:/Users/marco/Documents/ControlLab/ControlLab-Project/validation_report.md"
        
    def validate_dataset(self):
        """Valida estrutura e conteÃºdo do dataset"""
        print("ğŸ” Validando dataset ControlLab...")
        
        conversations = []
        with open(self.dataset_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                try:
                    conv = json.loads(line)
                    conversations.append(conv)
                except json.JSONDecodeError as e:
                    print(f"âŒ Erro JSON linha {line_num}: {e}")
                    return False
        
        # EstatÃ­sticas bÃ¡sicas
        total_conversations = len(conversations)
        print(f"ğŸ“Š Total de conversas: {total_conversations}")
        
        # Validar estrutura das conversas
        valid_conversations = 0
        conversation_types = Counter()
        message_lengths = []
        
        for i, conv in enumerate(conversations):
            if self._validate_conversation_structure(conv):
                valid_conversations += 1
                
                # Coletar metadados
                if 'metadata' in conv:
                    conv_type = conv['metadata'].get('type', 'unknown')
                    conversation_types[conv_type] += 1
                
                # Calcular comprimentos das mensagens
                for message in conv.get('messages', []):
                    message_lengths.append(len(message.get('content', '')))
            else:
                print(f"âŒ Conversa {i+1} com estrutura invÃ¡lida")
        
        validation_success = valid_conversations == total_conversations
        
        print(f"âœ… Conversas vÃ¡lidas: {valid_conversations}/{total_conversations}")
        print(f"ğŸ“ˆ Tipos de conversas: {dict(conversation_types)}")
        print(f"ğŸ“ Comprimento mÃ©dio das mensagens: {sum(message_lengths)/len(message_lengths):.0f} caracteres")
        
        return {
            'success': validation_success,
            'total_conversations': total_conversations,
            'valid_conversations': valid_conversations,
            'conversation_types': dict(conversation_types),
            'avg_message_length': sum(message_lengths)/len(message_lengths) if message_lengths else 0,
            'message_lengths': message_lengths
        }
    
    def _validate_conversation_structure(self, conv):
        """Valida estrutura de uma conversa individual"""
        required_keys = ['messages']
        
        # Verificar chaves obrigatÃ³rias
        for key in required_keys:
            if key not in conv:
                return False
        
        # Verificar estrutura das mensagens
        messages = conv['messages']
        if not isinstance(messages, list) or len(messages) < 2:
            return False
        
        # Verificar alternÃ¢ncia user/assistant
        expected_roles = ['user', 'assistant']
        for i, message in enumerate(messages):
            if not isinstance(message, dict):
                return False
            
            required_msg_keys = ['role', 'content']
            for key in required_msg_keys:
                if key not in message:
                    return False
            
            # Verificar role vÃ¡lido
            if message['role'] not in ['user', 'assistant', 'system']:
                return False
            
            # Verificar conteÃºdo nÃ£o vazio
            if not message['content'].strip():
                return False
        
        return True
    
    def analyze_content_quality(self):
        """Analisa qualidade do conteÃºdo das conversas"""
        print("ğŸ¯ Analisando qualidade do conteÃºdo...")
        
        conversations = []
        with open(self.dataset_path, 'r', encoding='utf-8') as f:
            for line in f:
                conversations.append(json.loads(line))
        
        # Palavras-chave relevantes para ControlLab
        controllab_keywords = [
            'controllab', 'sympy', 'simbÃ³lico', 'numÃ©rico', 'funÃ§Ã£o de transferÃªncia',
            'estabilidade', 'routh', 'hurwitz', 'lugar das raÃ­zes', 'bode',
            'nyquist', 'pid', 'controlador', 'pÃ³los', 'zeros', 'laplace',
            'resposta ao degrau', 'margem de fase', 'margem de ganho', 'tdah',
            'pedagÃ³gico', 'passo a passo'
        ]
        
        keyword_counts = Counter()
        pedagogical_indicators = []
        technical_depth_indicators = []
        
        for conv in conversations:
            content_text = ''
            for message in conv.get('messages', []):
                content_text += message.get('content', '').lower()
            
            # Contar palavras-chave
            for keyword in controllab_keywords:
                keyword_counts[keyword] += content_text.count(keyword)
            
            # Indicadores pedagÃ³gicos
            pedagogical_phrases = [
                'passo a passo', 'explicaÃ§Ã£o', 'por que', 'como funciona',
                'exemplo', 'vamos', 'primeiro', 'depois', 'finalmente'
            ]
            pedagogical_score = sum(content_text.count(phrase) for phrase in pedagogical_phrases)
            pedagogical_indicators.append(pedagogical_score)
            
            # Indicadores de profundidade tÃ©cnica
            technical_phrases = [
                'implementaÃ§Ã£o', 'cÃ³digo', 'funÃ§Ã£o', 'mÃ©todo', 'classe',
                'algoritmo', 'matemÃ¡tica', 'derivaÃ§Ã£o', 'cÃ¡lculo'
            ]
            technical_score = sum(content_text.count(phrase) for phrase in technical_phrases)
            technical_depth_indicators.append(technical_score)
        
        print(f"ğŸ”¤ Palavras-chave mais frequentes:")
        for keyword, count in keyword_counts.most_common(10):
            print(f"  â€¢ {keyword}: {count}")
        
        avg_pedagogical = sum(pedagogical_indicators) / len(pedagogical_indicators)
        avg_technical = sum(technical_depth_indicators) / len(technical_depth_indicators)
        
        print(f"ğŸ“ Score pedagÃ³gico mÃ©dio: {avg_pedagogical:.2f}")
        print(f"ğŸ”§ Score tÃ©cnico mÃ©dio: {avg_technical:.2f}")
        
        return {
            'keyword_counts': dict(keyword_counts),
            'avg_pedagogical_score': avg_pedagogical,
            'avg_technical_score': avg_technical,
            'pedagogical_scores': pedagogical_indicators,
            'technical_scores': technical_depth_indicators
        }
    
    def validate_config(self):
        """Valida configuraÃ§Ã£o de treinamento"""
        print("âš™ï¸ Validando configuraÃ§Ã£o de treinamento...")
        
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
        except Exception as e:
            print(f"âŒ Erro ao carregar configuraÃ§Ã£o: {e}")
            return False
        
        # Verificar seÃ§Ãµes obrigatÃ³rias
        required_sections = ['model_type', 'dataset', 'training', 'lora', 'output']
        for section in required_sections:
            if section not in config:
                print(f"âŒ SeÃ§Ã£o obrigatÃ³ria ausente: {section}")
                return False
        
        # Verificar parÃ¢metros crÃ­ticos
        critical_params = {
            'dataset.path': self.dataset_path,
            'training.batch_size': 4,
            'training.learning_rate': 1e-4,
            'lora.r': 64,
            'lora.alpha': 128
        }
        
        for param_path, expected_value in critical_params.items():
            keys = param_path.split('.')
            current_config = config
            
            try:
                for key in keys:
                    current_config = current_config[key]
                
                if param_path == 'dataset.path':
                    if not os.path.exists(current_config):
                        print(f"âŒ Dataset nÃ£o encontrado: {current_config}")
                        return False
                    print(f"âœ… {param_path}: {current_config}")
                else:
                    print(f"âœ… {param_path}: {current_config}")
                    
            except KeyError:
                print(f"âŒ ParÃ¢metro ausente: {param_path}")
                return False
        
        print("âœ… ConfiguraÃ§Ã£o vÃ¡lida!")
        return True
    
    def check_dependencies(self):
        """Verifica dependÃªncias necessÃ¡rias para o treinamento"""
        print("ğŸ“¦ Verificando dependÃªncias...")
        
        required_packages = [
            'torch', 'transformers', 'peft', 'datasets', 
            'accelerate', 'bitsandbytes', 'scipy', 'numpy'
        ]
        
        missing_packages = []
        installed_packages = []
        
        for package in required_packages:
            try:
                __import__(package)
                installed_packages.append(package)
                print(f"âœ… {package}")
            except ImportError:
                missing_packages.append(package)
                print(f"âŒ {package}")
        
        if missing_packages:
            print(f"\nğŸš¨ Instalar pacotes ausentes:")
            print(f"pip install {' '.join(missing_packages)}")
            return False
        else:
            print("âœ… Todas as dependÃªncias estÃ£o instaladas!")
            return True
    
    def estimate_training_time(self):
        """Estima tempo de treinamento"""
        print("â±ï¸ Estimando tempo de treinamento...")
        
        # Carregar configuraÃ§Ã£o
        with open(self.config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # ParÃ¢metros de treinamento
        batch_size = config['training']['batch_size']
        grad_accumulation = config['training']['gradient_accumulation_steps']
        num_epochs = config['training']['num_epochs']
        
        # Contar exemplos
        with open(self.dataset_path, 'r', encoding='utf-8') as f:
            num_examples = sum(1 for _ in f)
        
        effective_batch_size = batch_size * grad_accumulation
        steps_per_epoch = num_examples // effective_batch_size
        total_steps = steps_per_epoch * num_epochs
        
        # Estimativa baseada em hardware tÃ­pico (RTX 4090)
        seconds_per_step = 2.5  # Estimativa conservativa
        total_time_minutes = (total_steps * seconds_per_step) / 60
        
        print(f"ğŸ“Š Exemplos: {num_examples}")
        print(f"ğŸ”¢ Batch size efetivo: {effective_batch_size}")
        print(f"ğŸ“ˆ Steps por Ã©poca: {steps_per_epoch}")
        print(f"ğŸ¯ Total de steps: {total_steps}")
        print(f"â° Tempo estimado: {total_time_minutes:.1f} minutos")
        
        return {
            'num_examples': num_examples,
            'effective_batch_size': effective_batch_size,
            'steps_per_epoch': steps_per_epoch,
            'total_steps': total_steps,
            'estimated_time_minutes': total_time_minutes
        }
    
    def create_training_command(self):
        """Cria comando otimizado para treinamento"""
        command = """
# Comando para iniciar treinamento ControlLab
python train_controllab_assistant.py

# Para monitorar GPU:
# nvidia-smi -l 1

# Para verificar logs:
# tail -f logs/runs/*/events.out.tfevents.*

# Para interromper graciosamente:
# Ctrl+C (salva checkpoint automÃ¡tico)
"""
        
        print("ğŸš€ Comando de treinamento preparado:")
        print(command)
        return command
    
    def generate_validation_report(self):
        """Gera relatÃ³rio completo de validaÃ§Ã£o"""
        print("ğŸ“ Gerando relatÃ³rio de validaÃ§Ã£o...")
        
        # Executar todas as validaÃ§Ãµes
        dataset_validation = self.validate_dataset()
        content_analysis = self.analyze_content_quality()
        config_validation = self.validate_config()
        dependencies_check = self.check_dependencies()
        training_estimate = self.estimate_training_time()
        training_command = self.create_training_command()
        
        # Gerar relatÃ³rio markdown
        report_content = f"""# RelatÃ³rio de ValidaÃ§Ã£o - Fine-tuning ControlLab

**Data:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ¯ Resumo Executivo

O sistema de fine-tuning ControlLab foi configurado com sucesso para treinar um assistente pedagÃ³gico especializado em engenharia de controle.

### Status Geral
- âœ… Dataset: {dataset_validation['valid_conversations']}/{dataset_validation['total_conversations']} conversas vÃ¡lidas
- âœ… ConfiguraÃ§Ã£o: {'VÃ¡lida' if config_validation else 'InvÃ¡lida'}
- âœ… DependÃªncias: {'Instaladas' if dependencies_check else 'Ausentes'}
- â±ï¸ Tempo estimado: {training_estimate['estimated_time_minutes']:.1f} minutos

## ğŸ“Š AnÃ¡lise do Dataset

### EstatÃ­sticas BÃ¡sicas
- **Total de conversas:** {dataset_validation['total_conversations']}
- **Conversas vÃ¡lidas:** {dataset_validation['valid_conversations']}
- **Comprimento mÃ©dio:** {dataset_validation['avg_message_length']:.0f} caracteres

### DistribuiÃ§Ã£o por Tipo
{chr(10).join([f"- **{t}:** {c} conversas" for t, c in dataset_validation['conversation_types'].items()])}

### Qualidade do ConteÃºdo
- **Score pedagÃ³gico mÃ©dio:** {content_analysis['avg_pedagogical_score']:.2f}
- **Score tÃ©cnico mÃ©dio:** {content_analysis['avg_technical_score']:.2f}

### Palavras-chave mais Frequentes
{chr(10).join([f"- **{k}:** {v} ocorrÃªncias" for k, v in list(content_analysis['keyword_counts'].items())[:10]])}

## âš™ï¸ ConfiguraÃ§Ã£o de Treinamento

### ParÃ¢metros Principais
- **Modelo base:** Phi-3-mini-4k-instruct
- **MÃ©todo:** LoRA (Low-Rank Adaptation)
- **Batch size:** 4 (efetivo: {training_estimate['effective_batch_size']})
- **Learning rate:** 1e-4
- **Ã‰pocas:** 3

### OtimizaÃ§Ãµes LoRA
- **Rank (r):** 64
- **Alpha:** 128
- **Dropout:** 0.1
- **MÃ³dulos alvo:** q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj

## ğŸ“ EspecializaÃ§Ã£o PedagÃ³gica

### Foco TDAH-Friendly
- ExplicaÃ§Ãµes passo a passo
- Checkpoints de progresso
- Contexto sempre preservado
- MÃºltiplas representaÃ§Ãµes (visual, matemÃ¡tica, conceitual)

### Arquitetura SimbÃ³lico-NumÃ©rica
- NÃºcleo SymPy para precisÃ£o matemÃ¡tica
- Ponte python-control para simulaÃ§Ãµes
- DerivaÃ§Ãµes transparentes e validadas
- Interatividade em tempo real

## ğŸ“ˆ Estimativas de Treinamento

- **Exemplos totais:** {training_estimate['num_examples']}
- **Steps por Ã©poca:** {training_estimate['steps_per_epoch']}
- **Total de steps:** {training_estimate['total_steps']}
- **Tempo estimado:** {training_estimate['estimated_time_minutes']:.1f} minutos (~{training_estimate['estimated_time_minutes']/60:.1f} horas)

## ğŸš€ PrÃ³ximos Passos

1. **Iniciar Treinamento:**
   ```bash
   python train_controllab_assistant.py
   ```

2. **Monitorar Progresso:**
   - Verificar logs em `./logs/`
   - Monitorar GPU com `nvidia-smi`
   - Acompanhar mÃ©tricas de loss

3. **ValidaÃ§Ã£o PÃ³s-Treinamento:**
   - Testar respostas pedagÃ³gicas
   - Validar conhecimento ControlLab
   - Verificar suporte TDAH

4. **IntegraÃ§Ã£o Web:**
   - Carregar modelo na interface web
   - Configurar pipeline de inferÃªncia
   - Testar interaÃ§Ãµes em tempo real

## ğŸ’¡ ConsideraÃ§Ãµes Especiais

### Diferencial PedagÃ³gico
O modelo serÃ¡ treinado para:
- Explicar nÃ£o apenas "o quÃª", mas "por quÃª"
- Manter raciocÃ­nio persistente (ideal para TDAH)
- Conectar teoria com implementaÃ§Ã£o prÃ¡tica
- Fornecer mÃºltiplas perspectivas do mesmo conceito

### ValidaÃ§Ã£o de Qualidade
- Cada resposta deve incluir fundamentaÃ§Ã£o teÃ³rica
- CÃ³digo deve ser executÃ¡vel e pedagogicamente claro
- Exemplos devem conectar com material do livro Nise
- Suporte especÃ­fico para usuÃ¡rios com TDAH

**ğŸ¯ Meta:** Criar o primeiro assistente de IA especializado em ControlLab, capaz de ensinar engenharia de controle de forma inclusiva e pedagogicamente robusta.
"""
        
        # Salvar relatÃ³rio
        with open(self.validation_report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"âœ… RelatÃ³rio salvo: {self.validation_report_path}")
        return report_content
    
    def run_complete_validation(self):
        """Executa validaÃ§Ã£o completa do sistema"""
        print("ğŸ”¬ VALIDAÃ‡ÃƒO COMPLETA DO SISTEMA CONTROLLAB")
        print("=" * 60)
        
        try:
            # Executar validaÃ§Ãµes
            dataset_valid = self.validate_dataset()
            content_analysis = self.analyze_content_quality()
            config_valid = self.validate_config()
            deps_ok = self.check_dependencies()
            training_info = self.estimate_training_time()
            
            # Gerar relatÃ³rio
            report = self.generate_validation_report()
            
            # Status final
            all_valid = (
                dataset_valid['success'] and 
                config_valid and 
                deps_ok
            )
            
            print(f"\n{'âœ…' if all_valid else 'âŒ'} SISTEMA {'PRONTO' if all_valid else 'REQUER ATENÃ‡ÃƒO'}")
            
            if all_valid:
                print("\nğŸš€ PRONTO PARA TREINAMENTO!")
                print("Execute: python train_controllab_assistant.py")
            else:
                print("\nğŸ”§ Corrija os problemas identificados antes de treinar.")
            
            return {
                'ready_for_training': all_valid,
                'dataset_validation': dataset_valid,
                'content_analysis': content_analysis,
                'config_validation': config_valid,
                'dependencies_ok': deps_ok,
                'training_estimate': training_info,
                'report_path': self.validation_report_path
            }
            
        except Exception as e:
            print(f"âŒ Erro durante validaÃ§Ã£o: {e}")
            return {'ready_for_training': False, 'error': str(e)}

if __name__ == "__main__":
    validator = ControlLabFineTuningValidator()
    results = validator.run_complete_validation()
